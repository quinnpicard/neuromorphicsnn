{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import neuro\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import risp\n",
    "import eons\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wave\n",
    "import os \n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# original params\n",
    "if False:\n",
    "    eo_params = {\n",
    "        \"starting_nodes\": 3,\n",
    "        \"starting_edges\": 6,\n",
    "        \"merge_rate\": 0,\n",
    "        \"population_size\": 100,\n",
    "        \"multi_edges\": 0,\n",
    "        \"crossover_rate\": 0.5,\n",
    "        \"mutation_rate\": 0.9,\n",
    "        \"selection_type\": \"tournament\",\n",
    "        \"tournament_size_factor\": 0.1,\n",
    "        \"tournament_best_net_factor\": 0.9,\n",
    "        \"random_factor\": 0.05,\n",
    "        \"num_mutations\": 3,\n",
    "        \"node_mutations\": { \"Threshold\": 1.0 },\n",
    "        \"net_mutations\": { },\n",
    "        \"edge_mutations\": { \"Weight\": 0.5, \"Delay\": 0.5 },\n",
    "        \"num_best\" : 4\n",
    "    }\n",
    "\n",
    "eo_params = {\n",
    "    \"starting_nodes\": 3,\n",
    "    \"starting_edges\": 6,\n",
    "    \"merge_rate\": 0.1,\n",
    "    \"population_size\": 10,\n",
    "    \"multi_edges\": 0,\n",
    "    \"crossover_rate\": 0.5,\n",
    "    \"mutation_rate\": 0.8,\n",
    "    \"selection_type\": \"tournament\",\n",
    "    \"tournament_size_factor\": 0.1,\n",
    "    \"tournament_best_net_factor\": 0.9,\n",
    "    \"random_factor\": 0.05,\n",
    "    \"num_mutations\": 3,\n",
    "    \"node_mutations\": { \"Threshold\": 1.0 },\n",
    "    \"net_mutations\": { },\n",
    "    \"edge_mutations\": { \"Weight\": 0.5, \"Delay\": 0.5 },\n",
    "    \"num_best\" : 4\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique song length in major_triad is [1382.]\n",
      "unique song length in diminished is [1382.]\n",
      "unique song length in npy_files is []\n",
      "unique song length in minor_seventh is [1382.]\n",
      "unique song length in minor_triad is [1382.]\n",
      "unique song length in dom7_ninth is [1382.]\n",
      "unique song length in major_ninth is [1382.]\n",
      "unique song length in augmented is [1382.]\n",
      "unique song length in dom7_seventh is [1382.]\n",
      "unique song length in major_seventh is [1382.]\n",
      "unique song length in minor_ninth is [1382.]\n",
      "unique song length in sus2 is [1382.]\n",
      "unique song length in sus4 is [1382.]\n",
      "unique sample rates for all genres [44100.]\n",
      "minimum song length is 1382\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os \n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "unique_sample_rates = np.array([])\n",
    "min_song_len = float('inf')\n",
    "\n",
    "directory = 'chord_database'\n",
    "\n",
    "def find_peaks_per_channel(spectrum, threshold=0):\n",
    "    peaks = np.zeros_like(spectrum)\n",
    "    \n",
    "    for i in range(spectrum.shape[0]):\n",
    "        channel_data = spectrum[i, :]\n",
    "        channel_peaks, _ = find_peaks(channel_data, height=threshold)\n",
    "        peaks[i, channel_peaks] = 1\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "def mel_binary_mean(mel_spectrum):\n",
    "    # Computes the row-wise mean values (the mean of each channel)\n",
    "    channel_means = np.mean(mel_spectrum, axis=1)\n",
    "\n",
    "    # Applies thresholding to create binary representation\n",
    "    binary_spectrum = np.where(mel_spectrum >= channel_means[:, np.newaxis], 1, 0)\n",
    "\n",
    "    return binary_spectrum\n",
    "\n",
    "\n",
    "# Create a new folder to save the npy files\n",
    "new_folder = \"npy_files\"\n",
    "new_folder_path = os.path.join(directory, new_folder)\n",
    "os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "for folder_name in os.listdir(directory):\n",
    "    unique_song_len = np.array([])\n",
    "\n",
    "    folder_path = os.path.join(directory, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if the item is a directory\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.wav'):\n",
    "                try: \n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    audio_signal, sample_rate = librosa.load(file_path, sr=None)\n",
    "\n",
    "                    unique_sample_rates = np.append(unique_sample_rates, sample_rate)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    n_fft = 1024\n",
    "                    hop_length = n_fft // 16\n",
    "                    magnitude_spectrum = np.abs(librosa.stft(audio_signal, n_fft=n_fft, hop_length=hop_length))\n",
    "                    num_mels = 8\n",
    "                    mel_spectrum = librosa.feature.melspectrogram(\n",
    "                        sr=sample_rate,\n",
    "                        S=magnitude_spectrum,\n",
    "                        n_fft=n_fft,\n",
    "                        hop_length=hop_length,\n",
    "                        n_mels=num_mels\n",
    "                    )\n",
    "                    #mel_spectrum = mel_spectrum[:, :min_song_len]\n",
    "                    unique_song_len = np.append(unique_song_len, np.shape(mel_spectrum[0]))\n",
    "                    peak_spectrogram = mel_binary_mean(mel_spectrum)\n",
    "                    #peak_spectrogram = peak_spectrogram[:, :min_song_len]\n",
    "                    \n",
    "                    if min_song_len > len(peak_spectrogram[1]):\n",
    "                        min_song_len = len(peak_spectrogram[1])\n",
    "\n",
    "\n",
    "                    output_filename = f\"{filename}_peak_spectrogram.npy\"\n",
    "                    output_path = os.path.join(new_folder_path, output_filename)\n",
    "                    np.save(output_path, peak_spectrogram)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename} in folder {folder_name}: {str(e)}\")\n",
    "                    continue\n",
    "        print(f\"unique song length in {folder_name} is {np.unique(unique_song_len)}\")\n",
    "\n",
    "min_song_len = int(min_song_len)        \n",
    "print(f\"unique sample rates for all genres {np.unique(unique_sample_rates)}\")\n",
    "print(f\"minimum song length is {min_song_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_npy_files_with_prefix(directory, prefix, min_song_len):\n",
    "    npy_files = [file for file in os.listdir(directory) if file.startswith(prefix) and file.endswith('.npy')]\n",
    "    npy_files.sort()  # Sort the files for consistent order\n",
    "\n",
    "    if len(npy_files) == 0:\n",
    "        raise ValueError(f\"No npy files found with prefix '{prefix}' in directory '{directory}'\")\n",
    "    loaded_data = []\n",
    "\n",
    "    times_padded = 0\n",
    "    for npy_file in npy_files:\n",
    "        npy_path = os.path.join(directory, npy_file)\n",
    "        data = np.load(npy_path)\n",
    "\n",
    "        # Pad or trim the data array to the desired shape (min_song_len)\n",
    "   \n",
    "        if len(data[1]) > min_song_len:\n",
    "            trimmed_data = data[:, :min_song_len]\n",
    "            loaded_data.append(trimmed_data)\n",
    "        else:\n",
    "            loaded_data.append(data)\n",
    "\n",
    "    return np.array(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of major songs loaded: 183\n",
      "Number of minor songs loaded: 183\n"
     ]
    }
   ],
   "source": [
    "directory = new_folder_path\n",
    "\n",
    "# Loading classifier 1 data \n",
    "prefix_class1 = 'major'\n",
    "X_class1 = load_npy_files_with_prefix(directory, prefix_class1, min_song_len=min_song_len)\n",
    "y_class1 = ['major'] * len(X_class1)\n",
    "print(f\"Number of {prefix_class1} songs loaded: {len(X_class1)}\")\n",
    "\n",
    "# Loading classifier 2 data \n",
    "prefix_class2 = 'minor'\n",
    "X_class2 = load_npy_files_with_prefix(directory, prefix_class2, min_song_len=min_song_len)\n",
    "y_class2 = ['minor'] * len(X_class2)\n",
    "print(f\"Number of {prefix_class2} songs loaded: {len(X_class2)}\")\n",
    "\n",
    "# Combining the data and labels\n",
    "X = np.concatenate((X_class1, X_class2), axis=0)\n",
    "y = np.concatenate((y_class1, y_class2), axis=0)\n",
    "\n",
    "X = (np.rint(X)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n",
    "\n",
    "labels = np.unique(y_train)\n",
    "dmin = [np.min(X_train[i]) for i in range(X_train.shape[0])]\n",
    "dmax = [np.max(X_train[i]) for i in range(X_train.shape[0])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting sample scale \n",
    "for i in range(len(X_train)):\n",
    "    # mel region (20 total) corresponds to id \n",
    "    for j in range(len(X_train[i])):\n",
    "        # time bin selection \n",
    "        for k in range(len(X_train[i][j])): \n",
    "            if X_train[i][j][k] != 0:\n",
    "                spike = neuro.Spike(id=j,time=0,value=X_train[i][j][k])\n",
    "                proc .... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "risp_config = {\n",
    "  \"leak_mode\": True,\n",
    "  \"min_weight\": -1,\n",
    "  \"max_weight\": 1,\n",
    "  \"min_threshold\": -1,\n",
    "  \"max_threshold\": 1,\n",
    "  \"max_delay\": 5\n",
    "}\n",
    "\n",
    "proc = risp.Processor(risp_config)\n",
    "\n",
    "temp_net = neuro.Network()\n",
    "temp_net.set_properties(proc.get_network_properties())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neuron(neuron_id, net, moa):\n",
    "    neuron = net.add_node(neuron_id)\n",
    "    temp_net.randomize_node_properties(moa, neuron)\n",
    "    return neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = num_mels\n",
    "n_hidden = 20\n",
    "n_outputs = len(labels)\n",
    "n_neurons = n_inputs+n_hidden+n_outputs\n",
    "n_synapses = 120\n",
    "seed = 42\n",
    "\n",
    "moa = neuro.MOA()\n",
    "moa.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(n_inputs):\n",
    "    neuron = create_neuron(i, temp_net, moa)\n",
    "    neuron.set(\"Threshold\",0.75)\n",
    "    temp_net.add_input(neuron.id)\n",
    "    \n",
    "for i in range(n_outputs):\n",
    "    neuron = create_neuron(i+n_inputs, temp_net, moa)\n",
    "    neuron.set(\"Threshold\",0.75)\n",
    "    temp_net.add_output(neuron.id)\n",
    "    \n",
    "for i in range(n_hidden):\n",
    "    neuron = create_neuron(i+n_inputs+n_outputs, temp_net, moa)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_synapses):\n",
    "    source = random.randint(0,n_neurons-1)\n",
    "    dest = random.randint(0,n_neurons-1)\n",
    "    synapse = temp_net.add_or_get_edge(source, dest)\n",
    "    temp_net.randomize_edge_properties(moa, synapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolver = eons.EONS(eo_params)\n",
    "evolver.set_template_network(temp_net)\n",
    "\n",
    "pop = evolver.generate_population(eo_params,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x):\n",
    "    #print(x)\n",
    "    #proc.clear_activity()\n",
    "    for j in range(len(x)):\n",
    "            # time bin selection \n",
    "            for k in range(len(x[j])): \n",
    "                if x[j][k] != 0:\n",
    "                    spike = neuro.Spike(id=j,time=0,value=x[j][k])\n",
    "                    proc.apply_spike(spike)\n",
    "    proc.run(50)\n",
    "    return labels[proc.output_count_max(n_outputs)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(net, X, y):\n",
    "    proc.load_network(net)\n",
    "    \n",
    "    # Set up output tracking\n",
    "    for i in range(n_outputs):\n",
    "        proc.track_neuron_events(i)\n",
    "    \n",
    "    y_predict = [get_prediction(x) for x in X]\n",
    "    #print(len(y_predict))\n",
    "    return accuracy_score(y_predict, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define batch size and number of batches\\nbatch_size = 10\\nn_batches = len(X_train) // batch_size\\n\\n# Iterate over the batches\\nfor epoch in range(100):\\n    # Track the best performing network throughout and print the current best result\\n    best_fitness = 0.0\\n    mean_fitness = 0.0\\n\\n    # Process each batch\\n    for batch_idx in range(n_batches):\\n        # Get the batch data and labels\\n        batch_start = batch_idx * batch_size\\n        batch_end = (batch_idx + 1) * batch_size\\n        X_batch = X_train[batch_start:batch_end]\\n        y_batch = y_train[batch_start:batch_end]\\n\\n        # Calculate the fitnesses of all the networks in the population for the batch\\n        fitnesses = [fitness(net.network, X_batch, y_batch) for net in pop.networks]\\n\\n        # Update the best and mean fitness\\n        max_fit = max(fitnesses)\\n        mean_fit = np.mean(fitnesses)\\n        best_fitness = max(best_fitness, max_fit)\\n        mean_fitness += mean_fit\\n        print(\"Epoch:\", epoch, \"Batch:\", batch_idx, \"Best Fitness:\", best_fitness, \"Mean Fitness:\", mean_fitness)\\n\\n        # Create the next population based on the fitnesses of the current population for the batch\\n        pop = evolver.do_epoch(pop, fitnesses, eo_params)\\n\\n    # Calculate the mean fitness across all batches\\n    mean_fitness /= n_batches\\n\\n    # Print the progress for the epoch\\n    print(\"Epoch:\", epoch, \"Best Fitness:\", best_fitness, \"Mean Fitness:\", mean_fitness)\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define batch size and number of batches\n",
    "batch_size = 10\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "# Iterate over the batches\n",
    "for epoch in range(100):\n",
    "    # Track the best performing network throughout and print the current best result\n",
    "    best_fitness = 0.0\n",
    "    mean_fitness = 0.0\n",
    "\n",
    "    # Process each batch\n",
    "    for batch_idx in range(n_batches):\n",
    "        # Get the batch data and labels\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx + 1) * batch_size\n",
    "        X_batch = X_train[batch_start:batch_end]\n",
    "        y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "        # Calculate the fitnesses of all the networks in the population for the batch\n",
    "        fitnesses = [fitness(net.network, X_batch, y_batch) for net in pop.networks]\n",
    "\n",
    "        # Update the best and mean fitness\n",
    "        max_fit = max(fitnesses)\n",
    "        mean_fit = np.mean(fitnesses)\n",
    "        best_fitness = max(best_fitness, max_fit)\n",
    "        mean_fitness += mean_fit\n",
    "        print(\"Epoch:\", epoch, \"Batch:\", batch_idx, \"Best Fitness:\", best_fitness, \"Mean Fitness:\", mean_fitness)\n",
    "\n",
    "        # Create the next population based on the fitnesses of the current population for the batch\n",
    "        pop = evolver.do_epoch(pop, fitnesses, eo_params)\n",
    "\n",
    "    # Calculate the mean fitness across all batches\n",
    "    mean_fitness /= n_batches\n",
    "\n",
    "    # Print the progress for the epoch\n",
    "    print(\"Epoch:\", epoch, \"Best Fitness:\", best_fitness, \"Mean Fitness:\", mean_fitness)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  :  max fit 0.5020408163265306 mean fit 0.4983673469387755\n",
      "Epoch  1  :  max fit 0.5020408163265306 mean fit 0.4983673469387755\n",
      "Epoch  2  :  max fit 0.5020408163265306 mean fit 0.4983673469387755\n",
      "Epoch  3  :  max fit 0.5020408163265306 mean fit 0.4987755102040817\n",
      "Epoch  4  :  max fit 0.5020408163265306 mean fit 0.49918367346938775\n",
      "Epoch  5  :  max fit 0.5020408163265306 mean fit 0.5\n",
      "Epoch  6  :  max fit 0.5020408163265306 mean fit 0.5008163265306123\n",
      "Epoch  7  :  max fit 0.5020408163265306 mean fit 0.5012244897959184\n",
      "Epoch  8  :  max fit 0.5020408163265306 mean fit 0.5012244897959184\n",
      "Epoch  9  :  max fit 0.5020408163265306 mean fit 0.5008163265306123\n",
      "Epoch  10  :  max fit 0.5020408163265306 mean fit 0.5\n",
      "Epoch  11  :  max fit 0.5020408163265306 mean fit 0.5004081632653061\n",
      "Epoch  12  :  max fit 0.5020408163265306 mean fit 0.5\n",
      "Epoch  13  :  max fit 0.5020408163265306 mean fit 0.5004081632653061\n",
      "Epoch  14  :  max fit 0.5020408163265306 mean fit 0.5008163265306123\n",
      "Epoch  15  :  max fit 0.5020408163265306 mean fit 0.5012244897959184\n",
      "Epoch  16  :  max fit 0.5020408163265306 mean fit 0.5008163265306123\n",
      "Epoch  17  :  max fit 0.5020408163265306 mean fit 0.5008163265306124\n",
      "Epoch  18  :  max fit 0.5020408163265306 mean fit 0.5008163265306124\n",
      "Epoch  19  :  max fit 0.5020408163265306 mean fit 0.5004081632653061\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for i in range(25):\n",
    "    # Calculate the fitnesses of all of the networks in the population\n",
    "    fitnesses = [fitness(net.network, X_train, y_train) for net in pop.networks]\n",
    "    # Track the best performing network throughout and print the current best result\n",
    "    max_fit = max(fitnesses)\n",
    "    mean_fit = np.mean(fitnesses)\n",
    "    #print(fitnesses)\n",
    "    vals.append(max_fit)\n",
    "    print(\"Epoch \", i, \" : \",\"max fit\", max_fit, \"mean fit\",mean_fit)\n",
    "    \n",
    "    # Create the next population based on the fitnesses of the current population\n",
    "    pop = evolver.do_epoch(pop, fitnesses, eo_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.49795918367346936\n",
      "Testing Accuracy:  0.5041322314049587\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_net = pop.networks[fitnesses.index(max_fit)].network\n",
    "train = fitness(best_net, X_train, y_train)\n",
    "print(\"Training Accuracy: \", train)\n",
    "test = fitness(best_net, X_test, y_test)\n",
    "print(\"Testing Accuracy: \", test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
