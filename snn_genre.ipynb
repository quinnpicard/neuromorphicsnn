{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import neuro\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import risp\n",
    "import eons\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wave\n",
    "import os \n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# original params\n",
    "if False:\n",
    "    eo_params = {\n",
    "        \"starting_nodes\": 3,\n",
    "        \"starting_edges\": 6,\n",
    "        \"merge_rate\": 0,\n",
    "        \"population_size\": 100,\n",
    "        \"multi_edges\": 0,\n",
    "        \"crossover_rate\": 0.5,\n",
    "        \"mutation_rate\": 0.9,\n",
    "        \"selection_type\": \"tournament\",\n",
    "        \"tournament_size_factor\": 0.1,\n",
    "        \"tournament_best_net_factor\": 0.9,\n",
    "        \"random_factor\": 0.05,\n",
    "        \"num_mutations\": 3,\n",
    "        \"node_mutations\": { \"Threshold\": 1.0 },\n",
    "        \"net_mutations\": { },\n",
    "        \"edge_mutations\": { \"Weight\": 0.5, \"Delay\": 0.5 },\n",
    "        \"num_best\" : 4\n",
    "    }\n",
    "\n",
    "eo_params = {\n",
    "    \"starting_nodes\": 3,\n",
    "    \"starting_edges\": 6,\n",
    "    \"merge_rate\": 0.1,\n",
    "    \"population_size\": 100,\n",
    "    \"multi_edges\": 0,\n",
    "    \"crossover_rate\": 0.5,\n",
    "    \"mutation_rate\": .9,\n",
    "    \"selection_type\": \"tournament\",\n",
    "    \"tournament_size_factor\": 0.1,\n",
    "    \"tournament_best_net_factor\": 0.9,\n",
    "    \"random_factor\": 0.05,\n",
    "    \"num_mutations\": 20,\n",
    "    \"node_mutations\": { \"Threshold\": 1.0 },\n",
    "    \"net_mutations\": { },\n",
    "    \"edge_mutations\": { \"Weight\": 0.5, \"Delay\": 0.5 },\n",
    "    \"num_best\" : 4\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique song length in pop is [661504.]\n",
      "unique song length in metal is [661504. 661794.]\n",
      "unique song length in disco is [661344. 661504. 661676. 661760. 661794. 663520. 664180. 665060. 666160.\n",
      " 667920. 668140.]\n",
      "unique song length in npy_files is []\n",
      "unique song length in blues is [661794.]\n",
      "unique song length in reggae is [661504. 661794.]\n",
      "unique song length in classical is [661344. 661408. 661676. 661760. 661794. 663080. 663520. 665280. 669680.\n",
      " 670120. 672282.]\n",
      "unique song length in rock is [661408. 661500. 661794. 667920. 669460. 670340.]\n",
      "unique song length in hiphop is [660000. 661408. 661504. 661676. 661760. 661794. 664400. 665280. 667700.\n",
      " 668140. 669240. 669680. 675808.]\n",
      "unique song length in country is [661100. 661408. 661760. 661794. 663300. 663740. 666820. 668800. 669680.]\n",
      "Error processing jazz.00054.wav in folder jazz: \n",
      "unique song length in jazz is [661676. 661794. 661980. 665280. 665940. 666820. 667480. 669240. 672100.]\n",
      "unique sample rates for all genres [22050.]\n"
     ]
    }
   ],
   "source": [
    "unique_sample_rates = np.array([]) # <-------------\n",
    "\n",
    "\n",
    "\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os \n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "directory = 'audio_database'\n",
    "def find_peaks_per_channel(spectrum, threshold = 0):\n",
    "    peaks = np.zeros_like(spectrum)\n",
    "    \n",
    "    for i in range(spectrum.shape[0]):\n",
    "        channel_data = spectrum[i, :]\n",
    "        channel_peaks, _ = find_peaks(channel_data, height=threshold)\n",
    "        peaks[i, channel_peaks] = 1\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "# Create a new folder to save the npy files\n",
    "new_folder = \"npy_files\"\n",
    "new_folder_path = os.path.join(directory, new_folder)\n",
    "os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "for folder_name in os.listdir(directory):\n",
    "\n",
    "    unique_song_len = np.array([]) #<---------------\n",
    "    \n",
    "\n",
    "    folder_path = os.path.join(directory, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if the item is a directory\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.wav'):\n",
    "                try: \n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    audio_signal, sample_rate = librosa.load(file_path, sr=None) # we may want to hardcode our sampling rate if it is not consistant between songs \n",
    "                    \n",
    "                    unique_sample_rates = np.append(unique_sample_rates,sample_rate) # <----------\n",
    "                    unique_song_len = np.append(unique_song_len,np.shape(audio_signal)) # <----------\n",
    "\n",
    "                    n_fft = 1024\n",
    "                    hop_length = n_fft // 16 #number of audio samples between successive frames in the Short-Time Fourier Transform (STFT) analysis\n",
    "                    magnitude_spectrum = np.abs(librosa.stft(audio_signal, n_fft=n_fft, hop_length=hop_length))\n",
    "\n",
    "                    # Appling Mel filterbank from librosa \n",
    "                    num_mels = 20 #128 originally, but likely too much computation time and no longer actually follows any biological model\n",
    "                    mel_spectrum = librosa.feature.melspectrogram(\n",
    "                        sr=sample_rate,\n",
    "                        S=magnitude_spectrum,\n",
    "                        n_fft=n_fft,\n",
    "                        hop_length=hop_length,\n",
    "                        n_mels=num_mels\n",
    "                    )\n",
    "                    \n",
    "                    peak_spectrogram = find_peaks_per_channel(mel_spectrum)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    # Saving peak_spectrogram with a filename including the original file's name\n",
    "                    # this will overwrite any pre-existing files with the same name\n",
    "                    output_filename = f\"{filename}_peak_spectrogram.npy\"\n",
    "                    output_path = os.path.join(new_folder_path, output_filename)\n",
    "                    np.save(output_path, peak_spectrogram)\n",
    "                    \n",
    "                    #print(f\"processed {filename} in folder {folder_name}. Saved peak_spectrogram as {output_filename}\")\n",
    "\n",
    "                    \n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename} in folder {folder_name}: {str(e)}\")\n",
    "                    continue\n",
    "        print(f\"unique song length in {folder_name} is {np.unique(unique_song_len)}\") # <------------\n",
    "print(f\"unique sample rates for all genres {np.unique(unique_sample_rates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy_files_with_prefix(directory, prefix):\n",
    "    npy_files = [file for file in os.listdir(directory) if file.startswith(prefix) and file.endswith('.npy')]\n",
    "    npy_files.sort()  # Sort the files for consistent order\n",
    "    \n",
    "    if len(npy_files) == 0:\n",
    "        raise ValueError(f\"No npy files found with prefix '{prefix}' in directory '{directory}'\")\n",
    "    loaded_data = []\n",
    "    \n",
    "    for npy_file in npy_files:\n",
    "        npy_path = os.path.join(directory, npy_file)\n",
    "        data = np.load(npy_path)\n",
    "        loaded_data.append(data)\n",
    "    \n",
    "    return np.array(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndirectory = new_folder_path\\nprefix = \\'blues\\'\\n\\ntry:\\n    data_array = load_npy_files_with_prefix(directory, prefix)\\n    print(f\"Loaded {len(data_array)} npy files with prefix \\'{prefix}\\'\")\\n    print(\"Shape of the loaded array:\", data_array.shape)\\nexcept ValueError as e:\\n    print(str(e))\\ndata_array\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "directory = new_folder_path\n",
    "prefix = 'blues'\n",
    "\n",
    "try:\n",
    "    data_array = load_npy_files_with_prefix(directory, prefix)\n",
    "    print(f\"Loaded {len(data_array)} npy files with prefix '{prefix}'\")\n",
    "    print(\"Shape of the loaded array:\", data_array.shape)\n",
    "except ValueError as e:\n",
    "    print(str(e))\n",
    "data_array\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_folder_path\n",
    "\n",
    "\n",
    "# loading hiphop songs \n",
    "X = load_npy_files_with_prefix(directory, 'pop')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spike_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Defining training and testing data \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(spike_df\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmajor_data\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(X, np\u001b[39m.\u001b[39marray(spike_df\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mminor_data\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m      4\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile([\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m],(\u001b[39m13\u001b[39m,\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spike_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining training and testing data \n",
    "X = np.array(spike_df.get('major_data'))\n",
    "X = np.append(X, np.array(spike_df.get('minor_data')))\n",
    "y = np.tile([1,0],(13,1))\n",
    "y = np.append(y, np.tile([0,1],(13,1)),axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n",
    "\n",
    "#labels = sorted(np.unique(y_train))\n",
    "labels = [[1,0],[0,1]]\n",
    "\n",
    "dmin = [np.min(X_train[i]) for i in range(X_train.shape[0])]\n",
    "dmax = [np.max(X_train[i]) for i in range(X_train.shape[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio_database/npy_files'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting sample scale \n",
    "for i in range(len(X_train)):\n",
    "    # mel region (20 total) corresponds to id \n",
    "    for j in range(len(X_train[i])):\n",
    "        # time bin selection \n",
    "        for k in range(len(X_train[i][j])): \n",
    "            if X_train[i][j][k] != 0:\n",
    "                spike = neuro.Spike(id=j,time=0,value=X_train[i][j][k])\n",
    "                proc .... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risp_config = {\n",
    "  \"leak_mode\": True,\n",
    "  \"min_weight\": -1,\n",
    "  \"max_weight\": 1,\n",
    "  \"min_threshold\": -1,\n",
    "  \"max_threshold\": 1,\n",
    "  \"max_delay\": 5\n",
    "}\n",
    "\n",
    "proc = risp.Processor(risp_config)\n",
    "\n",
    "temp_net = neuro.Network()\n",
    "temp_net.set_properties(proc.get_network_properties())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neuron(neuron_id, net, moa):\n",
    "    neuron = net.add_node(neuron_id)\n",
    "    temp_net.randomize_node_properties(moa, neuron)\n",
    "    return neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 20\n",
    "n_hidden = 40\n",
    "n_outputs = len(labels)\n",
    "n_neurons = n_inputs+n_hidden+n_outputs\n",
    "n_synapses = 100\n",
    "seed = 42\n",
    "\n",
    "moa = neuro.MOA()\n",
    "moa.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for i in range(n_inputs):\n",
    "        neuron = create_neuron(i, temp_net, moa)\n",
    "        neuron.set(\"Threshold\",0.75)\n",
    "        temp_net.add_input(i)\n",
    "\n",
    "    for i in range(n_outputs):\n",
    "        neuron = create_neuron(i+n_inputs, temp_net, moa)\n",
    "        neuron.set(\"Threshold\",0.75)\n",
    "        temp_net.add_output(i)\n",
    "        \n",
    "    for i in range(n_hidden):\n",
    "        neuron = create_neuron(i+n_inputs+n_outputs, temp_net, moa)\n",
    "        \n",
    "#if False:\n",
    "for i in range(n_inputs):\n",
    "    neuron = create_neuron(i, temp_net, moa)\n",
    "    neuron.set(\"Threshold\",0.75)\n",
    "    temp_net.add_input(neuron.id)\n",
    "    \n",
    "for i in range(n_outputs):\n",
    "    neuron = create_neuron(i+n_inputs, temp_net, moa)\n",
    "    neuron.set(\"Threshold\",0.75)\n",
    "    temp_net.add_output(neuron.id)\n",
    "    \n",
    "for i in range(n_hidden):\n",
    "    neuron = create_neuron(i+n_inputs+n_outputs, temp_net, moa)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_synapses):\n",
    "    source = random.randint(0,n_neurons-1)\n",
    "    dest = random.randint(0,n_neurons-1)\n",
    "    synapse = temp_net.add_or_get_edge(source, dest)\n",
    "    temp_net.randomize_edge_properties(moa, synapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolver = eons.EONS(eo_params)\n",
    "evolver.set_template_network(temp_net)\n",
    "\n",
    "pop = evolver.generate_population(eo_params,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# included into fitness function \n",
    "def get_prediction(X):\n",
    "    proc.clear_activity()\n",
    "        # selecting sample scale \n",
    "    for i in range(len(X_train)):\n",
    "        # mel region (20 total) corresponds to id \n",
    "        for j in range(len(X_train[i])):\n",
    "            # time bin selection \n",
    "            for k in range(len(X_train[i][j])): \n",
    "                if X_train[i][j][k] != 0:\n",
    "                    spike = neuro.Spike(id=j,time=0,value=X_train[i][j][k])\n",
    "                    proc.apply_spike(spike)\n",
    "    proc.run(100)\n",
    "    return labels[proc.output_count_max(n_outputs)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def fitness(net, X, y):\n",
    "    proc.load_network(net)\n",
    "    proc.clear_activity()\n",
    "    for l in range(net.num_nodes()):\n",
    "        proc.track_neuron_events(l)\n",
    "    \n",
    "        # selecting sample scale \n",
    "    for i in range(len(X_train)):\n",
    "        # mel region (20 total) corresponds to id \n",
    "        for j in range(len(X_train[i])):\n",
    "            # time bin selection \n",
    "            for k in range(len(X_train[i][j])): \n",
    "                if X_train[i][j][k] != 0:\n",
    "                    spike = neuro.Spike(id=j,time=0,value=X_train[i][j][k])\n",
    "                    proc.apply_spike(spike)\n",
    "        #proc.run(100)\n",
    "        y_predict = labels[proc.output_count_max(n_outputs)[0]]\n",
    "        \n",
    "    \n",
    "        #y_predict = [get_prediction(x) for x in X]\n",
    "        return accuracy_score(y_predict, y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x):\n",
    "    #print(x)\n",
    "    #proc.clear_activity()\n",
    "    for j in range(len(x)):\n",
    "            # time bin selection \n",
    "            for k in range(len(x[j])): \n",
    "                if x[j][k] != 0:\n",
    "                    spike = neuro.Spike(id=j,time=0,value=x[j][k])\n",
    "                    proc.apply_spike(spike)\n",
    "    proc.run(50)\n",
    "    return labels[proc.output_count_max(n_outputs)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(net, X, y):\n",
    "    proc.load_network(net)\n",
    "    \n",
    "    # Set up output tracking\n",
    "    for i in range(n_outputs):\n",
    "        proc.track_neuron_events(i)\n",
    "    \n",
    "    y_predict = [get_prediction(x) for x in X]\n",
    "    #print(len(y_predict))\n",
    "    return accuracy_score(y_predict, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  :  max fit 0.5882352941176471 mean fit 0.4205882352941177\n",
      "Epoch  1  :  max fit 0.5882352941176471 mean fit 0.45000000000000007\n",
      "Epoch  2  :  max fit 0.5882352941176471 mean fit 0.49764705882352944\n",
      "Epoch  3  :  max fit 0.5882352941176471 mean fit 0.5229411764705881\n",
      "Epoch  4  :  max fit 0.5882352941176471 mean fit 0.5458823529411764\n",
      "Epoch  5  :  max fit 0.5882352941176471 mean fit 0.5458823529411764\n",
      "Epoch  6  :  max fit 0.5882352941176471 mean fit 0.5494117647058823\n",
      "Epoch  7  :  max fit 0.5882352941176471 mean fit 0.5423529411764706\n",
      "Epoch  8  :  max fit 0.5882352941176471 mean fit 0.5405882352941176\n",
      "Epoch  9  :  max fit 0.5882352941176471 mean fit 0.5294117647058821\n",
      "Epoch  10  :  max fit 0.5882352941176471 mean fit 0.5617647058823527\n",
      "Epoch  11  :  max fit 0.5882352941176471 mean fit 0.5517647058823529\n",
      "Epoch  12  :  max fit 0.5882352941176471 mean fit 0.5529411764705882\n",
      "Epoch  13  :  max fit 0.5882352941176471 mean fit 0.5617647058823528\n",
      "Epoch  14  :  max fit 0.9411764705882353 mean fit 0.556470588235294\n",
      "Epoch  15  :  max fit 0.9411764705882353 mean fit 0.5717647058823527\n",
      "Epoch  16  :  max fit 0.9411764705882353 mean fit 0.558235294117647\n",
      "Epoch  17  :  max fit 0.9411764705882353 mean fit 0.5529411764705882\n",
      "Epoch  18  :  max fit 0.9411764705882353 mean fit 0.5605882352941175\n",
      "Epoch  19  :  max fit 0.9411764705882353 mean fit 0.5758823529411764\n",
      "Epoch  20  :  max fit 0.9411764705882353 mean fit 0.6105882352941177\n",
      "Epoch  21  :  max fit 0.9411764705882353 mean fit 0.61\n",
      "Epoch  22  :  max fit 0.9411764705882353 mean fit 0.5976470588235293\n",
      "Epoch  23  :  max fit 0.9411764705882353 mean fit 0.6164705882352941\n",
      "Epoch  24  :  max fit 0.9411764705882353 mean fit 0.6094117647058822\n",
      "Epoch  25  :  max fit 0.9411764705882353 mean fit 0.6094117647058823\n",
      "Epoch  26  :  max fit 0.9411764705882353 mean fit 0.5964705882352941\n",
      "Epoch  27  :  max fit 0.9411764705882353 mean fit 0.5817647058823529\n",
      "Epoch  28  :  max fit 0.9411764705882353 mean fit 0.6235294117647059\n",
      "Epoch  29  :  max fit 0.9411764705882353 mean fit 0.5929411764705882\n",
      "Epoch  30  :  max fit 0.9411764705882353 mean fit 0.5811764705882352\n",
      "Epoch  31  :  max fit 0.9411764705882353 mean fit 0.5970588235294116\n",
      "Epoch  32  :  max fit 0.9411764705882353 mean fit 0.5900000000000001\n",
      "Epoch  33  :  max fit 0.9411764705882353 mean fit 0.6005882352941175\n",
      "Epoch  34  :  max fit 0.9411764705882353 mean fit 0.5941176470588235\n",
      "Epoch  35  :  max fit 0.9411764705882353 mean fit 0.5947058823529411\n",
      "Epoch  36  :  max fit 0.9411764705882353 mean fit 0.6117647058823529\n",
      "Epoch  37  :  max fit 0.9411764705882353 mean fit 0.5947058823529411\n",
      "Epoch  38  :  max fit 0.9411764705882353 mean fit 0.6317647058823529\n",
      "Epoch  39  :  max fit 0.9411764705882353 mean fit 0.6064705882352941\n",
      "Epoch  40  :  max fit 0.9411764705882353 mean fit 0.5976470588235293\n",
      "Epoch  41  :  max fit 0.9411764705882353 mean fit 0.6305882352941176\n",
      "Epoch  42  :  max fit 0.9411764705882353 mean fit 0.6541176470588235\n",
      "Epoch  43  :  max fit 0.9411764705882353 mean fit 0.6270588235294118\n",
      "Epoch  44  :  max fit 0.9411764705882353 mean fit 0.6052941176470586\n",
      "Epoch  45  :  max fit 0.9411764705882353 mean fit 0.6829411764705882\n",
      "Epoch  46  :  max fit 0.9411764705882353 mean fit 0.6976470588235294\n",
      "Epoch  47  :  max fit 0.9411764705882353 mean fit 0.6270588235294118\n",
      "Epoch  48  :  max fit 0.9411764705882353 mean fit 0.674705882352941\n",
      "Epoch  49  :  max fit 0.9411764705882353 mean fit 0.6394117647058821\n",
      "Epoch  50  :  max fit 0.9411764705882353 mean fit 0.6570588235294117\n",
      "Epoch  51  :  max fit 0.9411764705882353 mean fit 0.7029411764705881\n",
      "Epoch  52  :  max fit 0.9411764705882353 mean fit 0.6923529411764704\n",
      "Epoch  53  :  max fit 0.9411764705882353 mean fit 0.7405882352941177\n",
      "Epoch  54  :  max fit 0.9411764705882353 mean fit 0.7411764705882353\n",
      "Epoch  55  :  max fit 0.9411764705882353 mean fit 0.72\n",
      "Epoch  56  :  max fit 0.9411764705882353 mean fit 0.7041176470588235\n",
      "Epoch  57  :  max fit 0.9411764705882353 mean fit 0.7223529411764704\n",
      "Epoch  58  :  max fit 0.9411764705882353 mean fit 0.686470588235294\n",
      "Epoch  59  :  max fit 0.9411764705882353 mean fit 0.6752941176470589\n",
      "Epoch  60  :  max fit 0.9411764705882353 mean fit 0.7094117647058823\n",
      "Epoch  61  :  max fit 0.9411764705882353 mean fit 0.6847058823529413\n",
      "Epoch  62  :  max fit 0.9411764705882353 mean fit 0.7223529411764704\n",
      "Epoch  63  :  max fit 0.9411764705882353 mean fit 0.7205882352941179\n",
      "Epoch  64  :  max fit 0.9411764705882353 mean fit 0.7141176470588236\n",
      "Epoch  65  :  max fit 0.9411764705882353 mean fit 0.7370588235294118\n",
      "Epoch  66  :  max fit 0.9411764705882353 mean fit 0.6735294117647058\n",
      "Epoch  67  :  max fit 0.9411764705882353 mean fit 0.7370588235294115\n",
      "Epoch  68  :  max fit 0.9411764705882353 mean fit 0.7152941176470589\n",
      "Epoch  69  :  max fit 0.9411764705882353 mean fit 0.7158823529411765\n",
      "Epoch  70  :  max fit 0.9411764705882353 mean fit 0.7005882352941177\n",
      "Epoch  71  :  max fit 0.9411764705882353 mean fit 0.7135294117647057\n",
      "Epoch  72  :  max fit 0.9411764705882353 mean fit 0.713529411764706\n",
      "Epoch  73  :  max fit 0.9411764705882353 mean fit 0.7111764705882351\n",
      "Epoch  74  :  max fit 0.9411764705882353 mean fit 0.7588235294117647\n",
      "Epoch  75  :  max fit 0.9411764705882353 mean fit 0.7223529411764708\n",
      "Epoch  76  :  max fit 0.9411764705882353 mean fit 0.716470588235294\n",
      "Epoch  77  :  max fit 0.9411764705882353 mean fit 0.7123529411764707\n",
      "Epoch  78  :  max fit 0.9411764705882353 mean fit 0.75\n",
      "Epoch  79  :  max fit 0.9411764705882353 mean fit 0.7605882352941176\n",
      "Epoch  80  :  max fit 0.9411764705882353 mean fit 0.7452941176470587\n",
      "Epoch  81  :  max fit 0.9411764705882353 mean fit 0.7247058823529412\n",
      "Epoch  82  :  max fit 0.9411764705882353 mean fit 0.7182352941176471\n",
      "Epoch  83  :  max fit 0.9411764705882353 mean fit 0.7711764705882351\n",
      "Epoch  84  :  max fit 0.9411764705882353 mean fit 0.7741176470588235\n",
      "Epoch  85  :  max fit 0.9411764705882353 mean fit 0.7647058823529411\n",
      "Epoch  86  :  max fit 0.9411764705882353 mean fit 0.7511764705882351\n",
      "Epoch  87  :  max fit 0.9411764705882353 mean fit 0.7658823529411766\n",
      "Epoch  88  :  max fit 0.9411764705882353 mean fit 0.66\n",
      "Epoch  89  :  max fit 0.9411764705882353 mean fit 0.7776470588235292\n",
      "Epoch  90  :  max fit 0.9411764705882353 mean fit 0.7635294117647058\n",
      "Epoch  91  :  max fit 0.9411764705882353 mean fit 0.7111764705882352\n",
      "Epoch  92  :  max fit 0.9411764705882353 mean fit 0.77\n",
      "Epoch  93  :  max fit 0.9411764705882353 mean fit 0.72\n",
      "Epoch  94  :  max fit 0.9411764705882353 mean fit 0.748235294117647\n",
      "Epoch  95  :  max fit 0.9411764705882353 mean fit 0.7458823529411766\n",
      "Epoch  96  :  max fit 0.9411764705882353 mean fit 0.7394117647058823\n",
      "Epoch  97  :  max fit 0.9411764705882353 mean fit 0.7629411764705882\n",
      "Epoch  98  :  max fit 0.9411764705882353 mean fit 0.7529411764705884\n",
      "Epoch  99  :  max fit 0.9411764705882353 mean fit 0.7470588235294116\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for i in range(100):\n",
    "    # Calculate the fitnesses of all of the networks in the population\n",
    "    fitnesses = [fitness(net.network, X_train, y_train) for net in pop.networks]\n",
    "    \n",
    "    # Track the best performing network throughout and print the current best result\n",
    "    max_fit = max(fitnesses)\n",
    "    mean_fit = np.mean(fitnesses)\n",
    "    #print(fitnesses)\n",
    "    vals.append(max_fit)\n",
    "    print(\"Epoch \", i, \" : \",\"max fit\", max_fit, \"mean fit\",mean_fit)\n",
    "    \n",
    "    # Create the next population based on the fitnesses of the current population\n",
    "    pop = evolver.do_epoch(pop, fitnesses, eo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9411764705882353\n",
      "Testing Accuracy:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_net = pop.networks[fitnesses.index(max_fit)].network\n",
    "train = fitness(best_net, X_train, y_train)\n",
    "print(\"Training Accuracy: \", train)\n",
    "test = fitness(best_net, X_test, y_test)\n",
    "print(\"Testing Accuracy: \", test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
