{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import neuro\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import risp\n",
    "import eons\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wave\n",
    "import os \n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# original params\n",
    "if False:\n",
    "    eo_params = {\n",
    "        \"starting_nodes\": 3,\n",
    "        \"starting_edges\": 6,\n",
    "        \"merge_rate\": 0,\n",
    "        \"population_size\": 100,\n",
    "        \"multi_edges\": 0,\n",
    "        \"crossover_rate\": 0.5,\n",
    "        \"mutation_rate\": 0.9,\n",
    "        \"selection_type\": \"tournament\",\n",
    "        \"tournament_size_factor\": 0.1,\n",
    "        \"tournament_best_net_factor\": 0.9,\n",
    "        \"random_factor\": 0.05,\n",
    "        \"num_mutations\": 3,\n",
    "        \"node_mutations\": { \"Threshold\": 1.0 },\n",
    "        \"net_mutations\": { },\n",
    "        \"edge_mutations\": { \"Weight\": 0.5, \"Delay\": 0.5 },\n",
    "        \"num_best\" : 4\n",
    "    }\n",
    "\n",
    "eo_params = {\n",
    "    \"starting_nodes\": 3,\n",
    "    \"starting_edges\": 6,\n",
    "    \"merge_rate\": 0.1,\n",
    "    \"population_size\": 100,\n",
    "    \"multi_edges\": 0,\n",
    "    \"crossover_rate\": 0.5,\n",
    "    \"mutation_rate\": .9,\n",
    "    \"selection_type\": \"tournament\",\n",
    "    \"tournament_size_factor\": 0.1,\n",
    "    \"tournament_best_net_factor\": 0.9,\n",
    "    \"random_factor\": 0.05,\n",
    "    \"num_mutations\": 20,\n",
    "    \"node_mutations\": { \"Threshold\": 1.0 },\n",
    "    \"net_mutations\": { },\n",
    "    \"edge_mutations\": { \"Weight\": 0.5, \"Delay\": 0.5 },\n",
    "    \"num_best\" : 4\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique song length in pop is [661504.]\n",
      "unique song length in metal is [661504. 661794.]\n",
      "unique song length in disco is [661344. 661504. 661676. 661760. 661794. 663520. 664180. 665060. 666160.\n",
      " 667920. 668140.]\n",
      "unique song length in npy_files is []\n",
      "unique song length in blues is [661794.]\n",
      "unique song length in reggae is [661504. 661794.]\n",
      "unique song length in classical is [661344. 661408. 661676. 661760. 661794. 663080. 663520. 665280. 669680.\n",
      " 670120. 672282.]\n",
      "unique song length in rock is [661408. 661500. 661794. 667920. 669460. 670340.]\n",
      "unique song length in hiphop is [660000. 661408. 661504. 661676. 661760. 661794. 664400. 665280. 667700.\n",
      " 668140. 669240. 669680. 675808.]\n",
      "unique song length in country is [661100. 661408. 661760. 661794. 663300. 663740. 666820. 668800. 669680.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quinn/Documents/local_framework/framework/pyframework/lib/python3.10/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing jazz.00054.wav in folder jazz: \n",
      "unique song length in jazz is [661676. 661794. 661980. 665280. 665940. 666820. 667480. 669240. 672100.]\n",
      "unique sample rates for all genres [22050.]\n",
      "minimum song length is 660000\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os \n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "unique_sample_rates = np.array([])\n",
    "min_song_len = float('inf')\n",
    "\n",
    "directory = 'audio_database'\n",
    "\n",
    "def find_peaks_per_channel(spectrum, threshold=0):\n",
    "    peaks = np.zeros_like(spectrum)\n",
    "    \n",
    "    for i in range(spectrum.shape[0]):\n",
    "        channel_data = spectrum[i, :]\n",
    "        channel_peaks, _ = find_peaks(channel_data, height=threshold)\n",
    "        peaks[i, channel_peaks] = 1\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "# Create a new folder to save the npy files\n",
    "new_folder = \"npy_files\"\n",
    "new_folder_path = os.path.join(directory, new_folder)\n",
    "os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "for folder_name in os.listdir(directory):\n",
    "    unique_song_len = np.array([])\n",
    "\n",
    "    folder_path = os.path.join(directory, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if the item is a directory\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.wav'):\n",
    "                try: \n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    audio_signal, sample_rate = librosa.load(file_path, sr=None)\n",
    "\n",
    "                    unique_sample_rates = np.append(unique_sample_rates, sample_rate)\n",
    "                    unique_song_len = np.append(unique_song_len, np.shape(audio_signal))\n",
    "                    if len(audio_signal) < min_song_len:\n",
    "                        min_song_len = len(audio_signal)\n",
    "                    \n",
    "                    n_fft = 1024\n",
    "                    hop_length = n_fft // 16\n",
    "                    magnitude_spectrum = np.abs(librosa.stft(audio_signal, n_fft=n_fft, hop_length=hop_length))\n",
    "                    num_mels = 20\n",
    "                    mel_spectrum = librosa.feature.melspectrogram(\n",
    "                        sr=sample_rate,\n",
    "                        S=magnitude_spectrum,\n",
    "                        n_fft=n_fft,\n",
    "                        hop_length=hop_length,\n",
    "                        n_mels=num_mels\n",
    "                    )\n",
    "                    mel_spectrum = mel_spectrum[:, :min_song_len]\n",
    "                    \n",
    "                    peak_spectrogram = find_peaks_per_channel(mel_spectrum)\n",
    "                    peak_spectrogram = peak_spectrogram[:, :min_song_len]\n",
    "\n",
    "                    output_filename = f\"{filename}_peak_spectrogram.npy\"\n",
    "                    output_path = os.path.join(new_folder_path, output_filename)\n",
    "                    np.save(output_path, peak_spectrogram)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename} in folder {folder_name}: {str(e)}\")\n",
    "                    continue\n",
    "        print(f\"unique song length in {folder_name} is {np.unique(unique_song_len)}\")\n",
    "\n",
    "min_song_len = int(min_song_len)        \n",
    "print(f\"unique sample rates for all genres {np.unique(unique_sample_rates)}\")\n",
    "print(f\"minimum song length is {min_song_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_npy_files_with_prefix(directory, prefix, min_song_len):\n",
    "    npy_files = [file for file in os.listdir(directory) if file.startswith(prefix) and file.endswith('.npy')]\n",
    "    npy_files.sort()  # Sort the files for consistent order\n",
    "\n",
    "    if len(npy_files) == 0:\n",
    "        raise ValueError(f\"No npy files found with prefix '{prefix}' in directory '{directory}'\")\n",
    "    loaded_data = []\n",
    "\n",
    "    for npy_file in npy_files:\n",
    "        npy_path = os.path.join(directory, npy_file)\n",
    "        data = np.load(npy_path)\n",
    "\n",
    "        # Pad or trim the data array to the desired shape (min_song_len)\n",
    "        if data.shape[1] < min_song_len:\n",
    "            padded_data = np.pad(data, ((0, 0), (0, min_song_len - data.shape[1])), mode='constant')\n",
    "            loaded_data.append(padded_data)\n",
    "        elif data.shape[1] > min_song_len:\n",
    "            trimmed_data = data[:, :min_song_len]\n",
    "            loaded_data.append(trimmed_data)\n",
    "        else:\n",
    "            loaded_data.append(data)\n",
    "\n",
    "    return np.array(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndirectory = new_folder_path\\nprefix = \\'blues\\'\\n\\ntry:\\n    data_array = load_npy_files_with_prefix(directory, prefix)\\n    print(f\"Loaded {len(data_array)} npy files with prefix \\'{prefix}\\'\")\\n    print(\"Shape of the loaded array:\", data_array.shape)\\nexcept ValueError as e:\\n    print(str(e))\\ndata_array\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "directory = new_folder_path\n",
    "prefix = 'blues'\n",
    "\n",
    "try:\n",
    "    data_array = load_npy_files_with_prefix(directory, prefix)\n",
    "    print(f\"Loaded {len(data_array)} npy files with prefix '{prefix}'\")\n",
    "    print(\"Shape of the loaded array:\", data_array.shape)\n",
    "except ValueError as e:\n",
    "    print(str(e))\n",
    "data_array\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_folder_path\n",
    "\n",
    "# Loading hiphop songs\n",
    "X_hiphop = load_npy_files_with_prefix(directory, 'hiphop', min_song_len=min_song_len)\n",
    "y_hiphop = ['hiphop'] * len(X_hiphop)\n",
    "\n",
    "# Loading country songs\n",
    "X_country = load_npy_files_with_prefix(directory, 'country', min_song_len=min_song_len)\n",
    "y_country = ['country'] * len(X_country)\n",
    "\n",
    "# Combining the data and labels\n",
    "X = np.concatenate((X_hiphop, X_country), axis=0)\n",
    "y = np.concatenate((y_hiphop, y_country), axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n",
    "\n",
    "labels = np.unique(y_train)\n",
    "dmin = [np.min(X_train[i]) for i in range(X_train.shape[0])]\n",
    "dmax = [np.max(X_train[i]) for i in range(X_train.shape[0])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio_database/npy_files'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting sample scale \n",
    "for i in range(len(X_train)):\n",
    "    # mel region (20 total) corresponds to id \n",
    "    for j in range(len(X_train[i])):\n",
    "        # time bin selection \n",
    "        for k in range(len(X_train[i][j])): \n",
    "            if X_train[i][j][k] != 0:\n",
    "                spike = neuro.Spike(id=j,time=0,value=X_train[i][j][k])\n",
    "                proc .... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "risp_config = {\n",
    "  \"leak_mode\": True,\n",
    "  \"min_weight\": -1,\n",
    "  \"max_weight\": 1,\n",
    "  \"min_threshold\": -1,\n",
    "  \"max_threshold\": 1,\n",
    "  \"max_delay\": 5\n",
    "}\n",
    "\n",
    "proc = risp.Processor(risp_config)\n",
    "\n",
    "temp_net = neuro.Network()\n",
    "temp_net.set_properties(proc.get_network_properties())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neuron(neuron_id, net, moa):\n",
    "    neuron = net.add_node(neuron_id)\n",
    "    temp_net.randomize_node_properties(moa, neuron)\n",
    "    return neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 20\n",
    "n_hidden = 40\n",
    "n_outputs = len(labels)\n",
    "n_neurons = n_inputs+n_hidden+n_outputs\n",
    "n_synapses = 100\n",
    "seed = 42\n",
    "\n",
    "moa = neuro.MOA()\n",
    "moa.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for i in range(n_inputs):\n",
    "        neuron = create_neuron(i, temp_net, moa)\n",
    "        neuron.set(\"Threshold\",0.75)\n",
    "        temp_net.add_input(i)\n",
    "\n",
    "    for i in range(n_outputs):\n",
    "        neuron = create_neuron(i+n_inputs, temp_net, moa)\n",
    "        neuron.set(\"Threshold\",0.75)\n",
    "        temp_net.add_output(i)\n",
    "        \n",
    "    for i in range(n_hidden):\n",
    "        neuron = create_neuron(i+n_inputs+n_outputs, temp_net, moa)\n",
    "        \n",
    "#if False:\n",
    "for i in range(n_inputs):\n",
    "    neuron = create_neuron(i, temp_net, moa)\n",
    "    neuron.set(\"Threshold\",0.75)\n",
    "    temp_net.add_input(neuron.id)\n",
    "    \n",
    "for i in range(n_outputs):\n",
    "    neuron = create_neuron(i+n_inputs, temp_net, moa)\n",
    "    neuron.set(\"Threshold\",0.75)\n",
    "    temp_net.add_output(neuron.id)\n",
    "    \n",
    "for i in range(n_hidden):\n",
    "    neuron = create_neuron(i+n_inputs+n_outputs, temp_net, moa)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_synapses):\n",
    "    source = random.randint(0,n_neurons-1)\n",
    "    dest = random.randint(0,n_neurons-1)\n",
    "    synapse = temp_net.add_or_get_edge(source, dest)\n",
    "    temp_net.randomize_edge_properties(moa, synapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolver = eons.EONS(eo_params)\n",
    "evolver.set_template_network(temp_net)\n",
    "\n",
    "pop = evolver.generate_population(eo_params,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# included into fitness function \n",
    "def get_prediction(X):\n",
    "    proc.clear_activity()\n",
    "        # selecting sample scale \n",
    "    for i in range(len(X_train)):\n",
    "        # mel region (20 total) corresponds to id \n",
    "        for j in range(len(X_train[i])):\n",
    "            # time bin selection \n",
    "            for k in range(len(X_train[i][j])): \n",
    "                if X_train[i][j][k] != 0:\n",
    "                    spike = neuro.Spike(id=j,time=0,value=X_train[i][j][k])\n",
    "                    proc.apply_spike(spike)\n",
    "    proc.run(100)\n",
    "    return labels[proc.output_count_max(n_outputs)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def fitness(net, X, y):\n",
    "    proc.load_network(net)\n",
    "    proc.clear_activity()\n",
    "    for l in range(net.num_nodes()):\n",
    "        proc.track_neuron_events(l)\n",
    "    \n",
    "        # selecting sample scale \n",
    "    for i in range(len(X_train)):\n",
    "        # mel region (20 total) corresponds to id \n",
    "        for j in range(len(X_train[i])):\n",
    "            # time bin selection \n",
    "            for k in range(len(X_train[i][j])): \n",
    "                if X_train[i][j][k] != 0:\n",
    "                    spike = neuro.Spike(id=j,time=0,value=X_train[i][j][k])\n",
    "                    proc.apply_spike(spike)\n",
    "        #proc.run(100)\n",
    "        y_predict = labels[proc.output_count_max(n_outputs)[0]]\n",
    "        \n",
    "    \n",
    "        #y_predict = [get_prediction(x) for x in X]\n",
    "        return accuracy_score(y_predict, y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x):\n",
    "    #print(x)\n",
    "    #proc.clear_activity()\n",
    "    for j in range(len(x)):\n",
    "            # time bin selection \n",
    "            for k in range(len(x[j])): \n",
    "                if x[j][k] != 0:\n",
    "                    spike = neuro.Spike(id=j,time=0,value=x[j][k])\n",
    "                    proc.apply_spike(spike)\n",
    "    proc.run(50)\n",
    "    return labels[proc.output_count_max(n_outputs)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(net, X, y):\n",
    "    proc.load_network(net)\n",
    "    \n",
    "    # Set up output tracking\n",
    "    for i in range(n_outputs):\n",
    "        proc.track_neuron_events(i)\n",
    "    \n",
    "    y_predict = [get_prediction(x) for x in X]\n",
    "    #print(len(y_predict))\n",
    "    return accuracy_score(y_predict, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for i in range(100):\n",
    "    # Calculate the fitnesses of all of the networks in the population\n",
    "    fitnesses = [fitness(net.network, X_train, y_train) for net in pop.networks]\n",
    "    \n",
    "    # Track the best performing network throughout and print the current best result\n",
    "    max_fit = max(fitnesses)\n",
    "    mean_fit = np.mean(fitnesses)\n",
    "    #print(fitnesses)\n",
    "    vals.append(max_fit)\n",
    "    print(\"Epoch \", i, \" : \",\"max fit\", max_fit, \"mean fit\",mean_fit)\n",
    "    \n",
    "    # Create the next population based on the fitnesses of the current population\n",
    "    pop = evolver.do_epoch(pop, fitnesses, eo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9411764705882353\n",
      "Testing Accuracy:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_net = pop.networks[fitnesses.index(max_fit)].network\n",
    "train = fitness(best_net, X_train, y_train)\n",
    "print(\"Training Accuracy: \", train)\n",
    "test = fitness(best_net, X_test, y_test)\n",
    "print(\"Testing Accuracy: \", test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
