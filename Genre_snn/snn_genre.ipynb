{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import neuro\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import risp\n",
    "import eons\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wave\n",
    "import os \n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# original params\n",
    "if False:\n",
    "    eo_params = {\n",
    "        \"starting_nodes\": 3,\n",
    "        \"starting_edges\": 6,\n",
    "        \"merge_rate\": 0,\n",
    "        \"population_size\": 100,\n",
    "        \"multi_edges\": 0,\n",
    "        \"crossover_rate\": 0.5,\n",
    "        \"mutation_rate\": 0.9,\n",
    "        \"selection_type\": \"tournament\",\n",
    "        \"tournament_size_factor\": 0.1,\n",
    "        \"tournament_best_net_factor\": 0.9,\n",
    "        \"random_factor\": 0.05,\n",
    "        \"num_mutations\": 3,\n",
    "        \"node_mutations\": { \"Threshold\": 1.0 },\n",
    "        \"net_mutations\": { },\n",
    "        \"edge_mutations\": { \"Weight\": 0.5, \"Delay\": 0.5 },\n",
    "        \"num_best\" : 4\n",
    "    }\n",
    "\n",
    "eo_params = {\n",
    "    \"starting_nodes\": 3,\n",
    "    \"starting_edges\": 6,\n",
    "    \"merge_rate\": 0.1,\n",
    "    \"population_size\": 100,\n",
    "    \"multi_edges\": 0,\n",
    "    \"crossover_rate\": 0.5,\n",
    "    \"mutation_rate\": 0.8,\n",
    "    \"selection_type\": \"tournament\",\n",
    "    \"tournament_size_factor\": 0.1,\n",
    "    \"tournament_best_net_factor\": 0.9,\n",
    "    \"random_factor\": 0.05,\n",
    "    \"num_mutations\": 4,\n",
    "    \"node_mutations\": { \"Threshold\": 1.0 },\n",
    "    \"net_mutations\": { },\n",
    "    \"edge_mutations\": { \"Weight\": 0.5, \"Delay\": 0.5 },\n",
    "    \"num_best\" : 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique song length in pop is [5169.]\n",
      "unique song length in metal is [5169. 5171.]\n",
      "unique song length in disco is [5167. 5169. 5170. 5171. 5184. 5189. 5196. 5205. 5219. 5220.]\n",
      "unique song length in npy_files is []\n",
      "unique song length in blues is [5171.]\n",
      "unique song length in reggae is [5169. 5171.]\n",
      "unique song length in classical is [5167. 5168. 5170. 5171. 5181. 5184. 5198. 5232. 5236. 5253.]\n",
      "unique song length in rock is [5168. 5171. 5219. 5231. 5238.]\n",
      "unique song length in hiphop is [5157. 5168. 5169. 5170. 5171. 5191. 5198. 5217. 5220. 5229. 5232. 5280.]\n",
      "unique song length in country is [5165. 5168. 5171. 5183. 5186. 5210. 5226. 5232.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quinn/Documents/local_framework/framework/pyframework/lib/python3.10/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing jazz.00054.wav in folder jazz: \n",
      "unique song length in jazz is [5170. 5171. 5172. 5198. 5203. 5210. 5215. 5229. 5251.]\n",
      "unique sample rates for all genres [22050.]\n",
      "minimum song length is 5157\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os \n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "unique_sample_rates = np.array([])\n",
    "min_song_len = float('inf')\n",
    "\n",
    "directory = 'audio_database'\n",
    "\n",
    "def find_peaks_per_channel(spectrum, threshold=0):\n",
    "    peaks = np.zeros_like(spectrum)\n",
    "    \n",
    "    for i in range(spectrum.shape[0]):\n",
    "        channel_data = spectrum[i, :]\n",
    "        channel_peaks, _ = find_peaks(channel_data, height=threshold)\n",
    "        peaks[i, channel_peaks] = 1\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "# Create a new folder to save the npy files\n",
    "new_folder = \"npy_files\"\n",
    "new_folder_path = os.path.join(directory, new_folder)\n",
    "os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "for folder_name in os.listdir(directory):\n",
    "    unique_song_len = np.array([])\n",
    "\n",
    "    folder_path = os.path.join(directory, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if the item is a directory\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.wav'):\n",
    "                try: \n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    audio_signal, sample_rate = librosa.load(file_path, sr=None)\n",
    "\n",
    "                    unique_sample_rates = np.append(unique_sample_rates, sample_rate)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    n_fft = 2048\n",
    "                    hop_length = n_fft // 16\n",
    "                    magnitude_spectrum = np.abs(librosa.stft(audio_signal, n_fft=n_fft, hop_length=hop_length))\n",
    "                    num_mels = 8\n",
    "                    mel_spectrum = librosa.feature.melspectrogram(\n",
    "                        sr=sample_rate,\n",
    "                        S=magnitude_spectrum,\n",
    "                        n_fft=n_fft,\n",
    "                        hop_length=hop_length,\n",
    "                        n_mels=num_mels\n",
    "                    )\n",
    "                    #mel_spectrum = mel_spectrum[:, :min_song_len]\n",
    "                    unique_song_len = np.append(unique_song_len, np.shape(mel_spectrum[0]))\n",
    "                    peak_spectrogram = find_peaks_per_channel(mel_spectrum)\n",
    "                    #peak_spectrogram = peak_spectrogram[:, :min_song_len]\n",
    "\n",
    "                    output_filename = f\"{filename}_peak_spectrogram.npy\"  # Move this line here\n",
    "                    output_path = os.path.join(new_folder_path, output_filename)\n",
    "                    np.save(output_path, peak_spectrogram)\n",
    "                    print(f\"Processed {filename} in folder {folder_name}. Saved peak_spectrogram as {output_filename}\")\n",
    "                    \n",
    "                    \n",
    "                    if min_song_len > len(peak_spectrogram[1]):\n",
    "                        min_song_len = len(peak_spectrogram[1])\n",
    "\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename} in folder {folder_name}: {str(e)}\")\n",
    "                    continue\n",
    "        print(f\"unique song length in {folder_name} is {np.unique(unique_song_len)}\")\n",
    "\n",
    "min_song_len = int(min_song_len)        \n",
    "print(f\"unique sample rates for all genres {np.unique(unique_sample_rates)}\")\n",
    "print(f\"minimum song length is {min_song_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_npy_files_with_prefix(directory, prefix, min_song_len):\n",
    "    npy_files = [file for file in os.listdir(directory) if file.startswith(prefix) and file.endswith('.npy')]\n",
    "    npy_files.sort()  # Sort the files for consistent order\n",
    "\n",
    "    if len(npy_files) == 0:\n",
    "        raise ValueError(f\"No npy files found with prefix '{prefix}' in directory '{directory}'\")\n",
    "    loaded_data = []\n",
    "\n",
    "    times_padded = 0\n",
    "    for npy_file in npy_files:\n",
    "        npy_path = os.path.join(directory, npy_file)\n",
    "        data = np.load(npy_path)\n",
    "\n",
    "        # Pad or trim the data array to the desired shape (min_song_len)\n",
    "   \n",
    "        if len(data[1]) > min_song_len:\n",
    "            trimmed_data = data[:, :min_song_len]\n",
    "            loaded_data.append(trimmed_data)\n",
    "        else:\n",
    "            loaded_data.append(data)\n",
    "\n",
    "    return np.array(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_folder_path\n",
    "\n",
    "# Loading hiphop songs\n",
    "X_hiphop = load_npy_files_with_prefix(directory, 'hiphop', min_song_len=min_song_len)\n",
    "y_hiphop = ['hiphop'] * len(X_hiphop)\n",
    "\n",
    "# Loading country songs\n",
    "X_country = load_npy_files_with_prefix(directory, 'country', min_song_len=min_song_len)\n",
    "y_country = ['country'] * len(X_country)\n",
    "\n",
    "# Combining the data and labels\n",
    "X = np.concatenate((X_hiphop, X_country), axis=0)\n",
    "y = np.concatenate((y_hiphop, y_country), axis=0)\n",
    "\n",
    "X = (np.rint(X)).astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "labels = np.unique(y_train)\n",
    "dmin = [np.min(X_train[i]) for i in range(X_train.shape[0])]\n",
    "dmax = [np.max(X_train[i]) for i in range(X_train.shape[0])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting sample scale \n",
    "for i in range(len(X_train)):\n",
    "    # mel region (20 total) corresponds to id \n",
    "    for j in range(len(X_train[i])):\n",
    "        # time bin selection \n",
    "        for k in range(len(X_train[i][j])): \n",
    "            if X_train[i][j][k] != 0:\n",
    "                spike = neuro.Spike(id=j,time=0,value=X_train[i][j][k])\n",
    "                proc .... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "risp_config = {\n",
    "  \"leak_mode\": True,\n",
    "  \"min_weight\": -1,\n",
    "  \"max_weight\": 1,\n",
    "  \"min_threshold\": -1,\n",
    "  \"max_threshold\": 1,\n",
    "  \"max_delay\": 5\n",
    "}\n",
    "\n",
    "proc = risp.Processor(risp_config)\n",
    "\n",
    "temp_net = neuro.Network()\n",
    "temp_net.set_properties(proc.get_network_properties())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neuron(neuron_id, net, moa):\n",
    "    neuron = net.add_node(neuron_id)\n",
    "    temp_net.randomize_node_properties(moa, neuron)\n",
    "    return neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = num_mels\n",
    "n_hidden = 200\n",
    "n_outputs = len(labels)\n",
    "n_neurons = n_inputs+n_hidden+n_outputs\n",
    "n_synapses = 1200\n",
    "seed = 42\n",
    "\n",
    "moa = neuro.MOA()\n",
    "moa.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(n_inputs):\n",
    "    neuron = create_neuron(i, temp_net, moa)\n",
    "    neuron.set(\"Threshold\",0.75)\n",
    "    temp_net.add_input(neuron.id)\n",
    "    \n",
    "for i in range(n_outputs):\n",
    "    neuron = create_neuron(i+n_inputs, temp_net, moa)\n",
    "    neuron.set(\"Threshold\",0.75)\n",
    "    temp_net.add_output(neuron.id)\n",
    "    \n",
    "for i in range(n_hidden):\n",
    "    neuron = create_neuron(i+n_inputs+n_outputs, temp_net, moa)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_synapses):\n",
    "    source = random.randint(0,n_neurons-1)\n",
    "    dest = random.randint(0,n_neurons-1)\n",
    "    synapse = temp_net.add_or_get_edge(source, dest)\n",
    "    temp_net.randomize_edge_properties(moa, synapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolver = eons.EONS(eo_params)\n",
    "evolver.set_template_network(temp_net)\n",
    "\n",
    "pop = evolver.generate_population(eo_params,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x):\n",
    "    #print(x)\n",
    "    #proc.clear_activity()\n",
    "    for j in range(len(x)):\n",
    "            # time bin selection \n",
    "            for k in range(len(x[j])): \n",
    "                if x[j][k] != 0:\n",
    "                    spike = neuro.Spike(id=j,time=k,value=x[j][k])\n",
    "                    #spike = neuro.Spike(id=j,time=0,value=x[j][k])\n",
    "                    proc.apply_spike(spike)\n",
    "    #proc.run(50)\n",
    "    proc.run(10000)\n",
    "    return labels[proc.output_count_max(n_outputs)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(net, X, y):\n",
    "    proc.load_network(net)\n",
    "    \n",
    "    # Set up output tracking\n",
    "    for i in range(n_outputs):\n",
    "        proc.track_neuron_events(i)\n",
    "    \n",
    "    y_predict = [get_prediction(x) for x in X]\n",
    "    #print(len(y_predict))\n",
    "    return accuracy_score(y_predict, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define batch size and number of batches\\nbatch_size = 10\\nn_batches = len(X_train) // batch_size\\n\\n# Iterate over the batches\\nfor epoch in range(100):\\n    # Track the best performing network throughout and print the current best result\\n    best_fitness = 0.0\\n    mean_fitness = 0.0\\n\\n    # Process each batch\\n    for batch_idx in range(n_batches):\\n        # Get the batch data and labels\\n        batch_start = batch_idx * batch_size\\n        batch_end = (batch_idx + 1) * batch_size\\n        X_batch = X_train[batch_start:batch_end]\\n        y_batch = y_train[batch_start:batch_end]\\n\\n        # Calculate the fitnesses of all the networks in the population for the batch\\n        fitnesses = [fitness(net.network, X_batch, y_batch) for net in pop.networks]\\n\\n        # Update the best and mean fitness\\n        max_fit = max(fitnesses)\\n        mean_fit = np.mean(fitnesses)\\n        best_fitness = max(best_fitness, max_fit)\\n        mean_fitness += mean_fit\\n        print(\"Epoch:\", epoch, \"Batch:\", batch_idx, \"Best Fitness:\", best_fitness, \"Mean Fitness:\", mean_fitness)\\n\\n        # Create the next population based on the fitnesses of the current population for the batch\\n        pop = evolver.do_epoch(pop, fitnesses, eo_params)\\n\\n    # Calculate the mean fitness across all batches\\n    mean_fitness /= n_batches\\n\\n    # Print the progress for the epoch\\n    print(\"Epoch:\", epoch, \"Best Fitness:\", best_fitness, \"Mean Fitness:\", mean_fitness)\\n'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define batch size and number of batches\n",
    "batch_size = 10\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "# Iterate over the batches\n",
    "for epoch in range(100):\n",
    "    # Track the best performing network throughout and print the current best result\n",
    "    best_fitness = 0.0\n",
    "    mean_fitness = 0.0\n",
    "\n",
    "    # Process each batch\n",
    "    for batch_idx in range(n_batches):\n",
    "        # Get the batch data and labels\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx + 1) * batch_size\n",
    "        X_batch = X_train[batch_start:batch_end]\n",
    "        y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "        # Calculate the fitnesses of all the networks in the population for the batch\n",
    "        fitnesses = [fitness(net.network, X_batch, y_batch) for net in pop.networks]\n",
    "\n",
    "        # Update the best and mean fitness\n",
    "        max_fit = max(fitnesses)\n",
    "        mean_fit = np.mean(fitnesses)\n",
    "        best_fitness = max(best_fitness, max_fit)\n",
    "        mean_fitness += mean_fit\n",
    "        print(\"Epoch:\", epoch, \"Batch:\", batch_idx, \"Best Fitness:\", best_fitness, \"Mean Fitness:\", mean_fitness)\n",
    "\n",
    "        # Create the next population based on the fitnesses of the current population for the batch\n",
    "        pop = evolver.do_epoch(pop, fitnesses, eo_params)\n",
    "\n",
    "    # Calculate the mean fitness across all batches\n",
    "    mean_fitness /= n_batches\n",
    "\n",
    "    # Print the progress for the epoch\n",
    "    print(\"Epoch:\", epoch, \"Best Fitness:\", best_fitness, \"Mean Fitness:\", mean_fitness)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  :  max fit 0.5746268656716418 mean fit 0.515223880597015\n",
      "Epoch  1  :  max fit 0.5970149253731343 mean fit 0.5172388059701493\n",
      "Epoch  2  :  max fit 0.5970149253731343 mean fit 0.5279850746268656\n",
      "Epoch  3  :  max fit 0.6119402985074627 mean fit 0.5561194029850746\n",
      "Epoch  4  :  max fit 0.6716417910447762 mean fit 0.5720895522388059\n",
      "Epoch  5  :  max fit 0.6716417910447762 mean fit 0.5776119402985074\n",
      "Epoch  6  :  max fit 0.6716417910447762 mean fit 0.5875373134328359\n",
      "Epoch  7  :  max fit 0.6716417910447762 mean fit 0.6165671641791046\n",
      "Epoch  8  :  max fit 0.6940298507462687 mean fit 0.6129104477611942\n",
      "Epoch  9  :  max fit 0.6940298507462687 mean fit 0.6246268656716418\n",
      "Epoch  10  :  max fit 0.7238805970149254 mean fit 0.6250746268656716\n",
      "Epoch  11  :  max fit 0.7238805970149254 mean fit 0.6344776119402985\n",
      "Epoch  12  :  max fit 0.7388059701492538 mean fit 0.6374626865671641\n",
      "Epoch  13  :  max fit 0.7388059701492538 mean fit 0.6443283582089552\n",
      "Epoch  14  :  max fit 0.7686567164179104 mean fit 0.6473134328358208\n",
      "Epoch  15  :  max fit 0.7686567164179104 mean fit 0.6416417910447761\n",
      "Epoch  16  :  max fit 0.7686567164179104 mean fit 0.6670149253731341\n",
      "Epoch  17  :  max fit 0.7686567164179104 mean fit 0.6773134328358209\n",
      "Epoch  18  :  max fit 0.7686567164179104 mean fit 0.6576865671641792\n",
      "Epoch  19  :  max fit 0.7686567164179104 mean fit 0.6738059701492536\n",
      "Epoch  20  :  max fit 0.7686567164179104 mean fit 0.6597014925373135\n",
      "Epoch  21  :  max fit 0.7686567164179104 mean fit 0.6516417910447763\n",
      "Epoch  22  :  max fit 0.7686567164179104 mean fit 0.6763432835820895\n",
      "Epoch  23  :  max fit 0.7686567164179104 mean fit 0.695820895522388\n",
      "Epoch  24  :  max fit 0.7686567164179104 mean fit 0.6748507462686568\n",
      "Epoch  25  :  max fit 0.7686567164179104 mean fit 0.6764925373134328\n",
      "Epoch  26  :  max fit 0.7686567164179104 mean fit 0.6611940298507463\n",
      "Epoch  27  :  max fit 0.7686567164179104 mean fit 0.6922388059701493\n",
      "Epoch  28  :  max fit 0.7686567164179104 mean fit 0.6931343283582091\n",
      "Epoch  29  :  max fit 0.7686567164179104 mean fit 0.6823880597014924\n",
      "Epoch  30  :  max fit 0.7686567164179104 mean fit 0.6751492537313433\n",
      "Epoch  31  :  max fit 0.7686567164179104 mean fit 0.7094029850746267\n",
      "Epoch  32  :  max fit 0.7686567164179104 mean fit 0.6845522388059705\n",
      "Epoch  33  :  max fit 0.7686567164179104 mean fit 0.6872388059701493\n",
      "Epoch  34  :  max fit 0.7686567164179104 mean fit 0.6942537313432835\n",
      "Epoch  35  :  max fit 0.7686567164179104 mean fit 0.6919402985074626\n",
      "Epoch  36  :  max fit 0.7686567164179104 mean fit 0.6982835820895522\n",
      "Epoch  37  :  max fit 0.7686567164179104 mean fit 0.698731343283582\n",
      "Epoch  38  :  max fit 0.7686567164179104 mean fit 0.7017910447761194\n",
      "Epoch  39  :  max fit 0.7686567164179104 mean fit 0.7087313432835822\n",
      "Epoch  40  :  max fit 0.7686567164179104 mean fit 0.7040298507462686\n",
      "Epoch  41  :  max fit 0.7686567164179104 mean fit 0.7120149253731343\n",
      "Epoch  42  :  max fit 0.7686567164179104 mean fit 0.7005970149253732\n",
      "Epoch  43  :  max fit 0.7686567164179104 mean fit 0.712462686567164\n",
      "Epoch  44  :  max fit 0.7686567164179104 mean fit 0.7036567164179104\n",
      "Epoch  45  :  max fit 0.7686567164179104 mean fit 0.7109701492537313\n",
      "Epoch  46  :  max fit 0.7686567164179104 mean fit 0.7261194029850746\n",
      "Epoch  47  :  max fit 0.7686567164179104 mean fit 0.7225373134328359\n",
      "Epoch  48  :  max fit 0.7686567164179104 mean fit 0.7056716417910447\n",
      "Epoch  49  :  max fit 0.7686567164179104 mean fit 0.6984328358208955\n",
      "Epoch  50  :  max fit 0.7686567164179104 mean fit 0.711641791044776\n",
      "Epoch  51  :  max fit 0.7686567164179104 mean fit 0.7014179104477613\n",
      "Epoch  52  :  max fit 0.7686567164179104 mean fit 0.7167910447761194\n",
      "Epoch  53  :  max fit 0.7686567164179104 mean fit 0.705223880597015\n",
      "Epoch  54  :  max fit 0.7686567164179104 mean fit 0.7217910447761193\n",
      "Epoch  55  :  max fit 0.7686567164179104 mean fit 0.704776119402985\n",
      "Epoch  56  :  max fit 0.7686567164179104 mean fit 0.7113432835820894\n",
      "Epoch  57  :  max fit 0.7761194029850746 mean fit 0.7047014925373135\n",
      "Epoch  58  :  max fit 0.7761194029850746 mean fit 0.7051492537313432\n",
      "Epoch  59  :  max fit 0.7761194029850746 mean fit 0.7039552238805971\n",
      "Epoch  60  :  max fit 0.7761194029850746 mean fit 0.7232835820895522\n",
      "Epoch  61  :  max fit 0.7761194029850746 mean fit 0.7151492537313432\n",
      "Epoch  62  :  max fit 0.7761194029850746 mean fit 0.720820895522388\n",
      "Epoch  63  :  max fit 0.7761194029850746 mean fit 0.7219402985074627\n",
      "Epoch  64  :  max fit 0.7761194029850746 mean fit 0.7309701492537313\n",
      "Epoch  65  :  max fit 0.7761194029850746 mean fit 0.7282089552238804\n",
      "Epoch  66  :  max fit 0.7761194029850746 mean fit 0.729776119402985\n",
      "Epoch  67  :  max fit 0.7761194029850746 mean fit 0.726044776119403\n",
      "Epoch  68  :  max fit 0.7761194029850746 mean fit 0.7339552238805968\n",
      "Epoch  69  :  max fit 0.7761194029850746 mean fit 0.743507462686567\n",
      "Epoch  70  :  max fit 0.7761194029850746 mean fit 0.7257462686567163\n",
      "Epoch  71  :  max fit 0.7761194029850746 mean fit 0.7267910447761193\n",
      "Epoch  72  :  max fit 0.7761194029850746 mean fit 0.7192537313432834\n",
      "Epoch  73  :  max fit 0.7761194029850746 mean fit 0.7252985074626863\n",
      "Epoch  74  :  max fit 0.7761194029850746 mean fit 0.7311940298507461\n",
      "Epoch  75  :  max fit 0.7761194029850746 mean fit 0.7285820895522386\n",
      "Epoch  76  :  max fit 0.7761194029850746 mean fit 0.7296268656716417\n",
      "Epoch  77  :  max fit 0.7761194029850746 mean fit 0.7447014925373133\n",
      "Epoch  78  :  max fit 0.7761194029850746 mean fit 0.7353731343283582\n",
      "Epoch  79  :  max fit 0.7761194029850746 mean fit 0.7295522388059701\n",
      "Epoch  80  :  max fit 0.7761194029850746 mean fit 0.7247014925373133\n",
      "Epoch  81  :  max fit 0.7761194029850746 mean fit 0.7350746268656715\n",
      "Epoch  82  :  max fit 0.7761194029850746 mean fit 0.7346268656716417\n",
      "Epoch  83  :  max fit 0.7761194029850746 mean fit 0.7217164179104475\n",
      "Epoch  84  :  max fit 0.7761194029850746 mean fit 0.7195522388059701\n",
      "Epoch  85  :  max fit 0.7835820895522388 mean fit 0.7219402985074627\n",
      "Epoch  86  :  max fit 0.7835820895522388 mean fit 0.7328358208955222\n",
      "Epoch  87  :  max fit 0.7835820895522388 mean fit 0.7179104477611937\n",
      "Epoch  88  :  max fit 0.7835820895522388 mean fit 0.7229850746268656\n",
      "Epoch  89  :  max fit 0.7835820895522388 mean fit 0.7285074626865672\n",
      "Epoch  90  :  max fit 0.7835820895522388 mean fit 0.7305970149253731\n",
      "Epoch  91  :  max fit 0.7835820895522388 mean fit 0.7471641791044776\n",
      "Epoch  92  :  max fit 0.7835820895522388 mean fit 0.7291791044776119\n",
      "Epoch  93  :  max fit 0.7835820895522388 mean fit 0.7406716417910448\n",
      "Epoch  94  :  max fit 0.7835820895522388 mean fit 0.733731343283582\n",
      "Epoch  95  :  max fit 0.7835820895522388 mean fit 0.7280597014925373\n",
      "Epoch  96  :  max fit 0.7835820895522388 mean fit 0.7358208955223879\n",
      "Epoch  97  :  max fit 0.7835820895522388 mean fit 0.7332835820895522\n",
      "Epoch  98  :  max fit 0.7835820895522388 mean fit 0.731044776119403\n",
      "Epoch  99  :  max fit 0.7835820895522388 mean fit 0.7411194029850746\n",
      "Epoch  100  :  max fit 0.7835820895522388 mean fit 0.7261940298507461\n",
      "Epoch  101  :  max fit 0.7835820895522388 mean fit 0.7364925373134329\n",
      "Epoch  102  :  max fit 0.7835820895522388 mean fit 0.7451492537313433\n",
      "Epoch  103  :  max fit 0.7835820895522388 mean fit 0.7365671641791046\n",
      "Epoch  104  :  max fit 0.7835820895522388 mean fit 0.7394029850746268\n",
      "Epoch  105  :  max fit 0.7835820895522388 mean fit 0.748955223880597\n",
      "Epoch  106  :  max fit 0.7835820895522388 mean fit 0.743955223880597\n",
      "Epoch  107  :  max fit 0.7835820895522388 mean fit 0.7443283582089552\n",
      "Epoch  108  :  max fit 0.7835820895522388 mean fit 0.7390298507462687\n",
      "Epoch  109  :  max fit 0.7835820895522388 mean fit 0.7479850746268656\n",
      "Epoch  110  :  max fit 0.7835820895522388 mean fit 0.7386567164179104\n",
      "Epoch  111  :  max fit 0.7835820895522388 mean fit 0.7479104477611941\n",
      "Epoch  112  :  max fit 0.7835820895522388 mean fit 0.7446268656716418\n",
      "Epoch  113  :  max fit 0.7835820895522388 mean fit 0.7439552238805969\n",
      "Epoch  114  :  max fit 0.7835820895522388 mean fit 0.7406716417910448\n",
      "Epoch  115  :  max fit 0.7835820895522388 mean fit 0.7411940298507461\n",
      "Epoch  116  :  max fit 0.7835820895522388 mean fit 0.7521641791044776\n",
      "Epoch  117  :  max fit 0.7835820895522388 mean fit 0.750223880597015\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for i in range(500):\n",
    "    # Calculate the fitnesses of all of the networks in the population\n",
    "    fitnesses = [fitness(net.network, X_train, y_train) for net in pop.networks]\n",
    "    # Track the best performing network throughout and print the current best result\n",
    "    max_fit = max(fitnesses)\n",
    "    mean_fit = np.mean(fitnesses)\n",
    "    #print(fitnesses)\n",
    "    vals.append(max_fit)\n",
    "    print(\"Epoch \", i, \" : \",\"max fit\", max_fit, \"mean fit\",mean_fit)\n",
    "    \n",
    "    # Create the next population based on the fitnesses of the current population\n",
    "    pop = evolver.do_epoch(pop, fitnesses, eo_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.746268656716418\n",
      "Testing Accuracy:  0.5757575757575758\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_net = pop.networks[fitnesses.index(max_fit)].network\n",
    "train = fitness(best_net, X_train, y_train)\n",
    "print(\"Training Accuracy: \", train)\n",
    "test = fitness(best_net, X_test, y_test)\n",
    "print(\"Testing Accuracy: \", test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
